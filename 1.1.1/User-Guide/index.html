<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>User Guide - PMem Shuffle - 1.1.1</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "User Guide";
    var mkdocs_page_input_path = "User-Guide.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> PMem Shuffle - 1.1.1</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">User Guide</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#1-pmem-shuffle-introduction">1. PMem Shuffle introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-recommended-hw-environment">2. Recommended HW environment</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#21-system-configuration">2.1. System Configuration</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#211-hw-and-sw-configuration">2.1.1 HW and SW Configuration</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#22-recommended-rdma-nic">2.2. Recommended RDMA NIC</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#23-recommended-pmem-configuration">2.3 Recommended PMEM configuration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#24-recommended-pmem-bkc-optional">2.4 Recommended PMEM BKC (optional)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#3-install-and-configure-pmem-example">3. Install and configure PMEM (example)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#4-configure-and-validate-rdma">4. Configure and Validate RDMA</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#41-configure-and-test-iwarp-rdma">4.1 Configure and test iWARP RDMA</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#411-download-rdma-core-and-install-dependencies">4.1.1 Download rdma-core and install dependencies</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#412-switch-configuration-optional">4.1.2 Switch Configuration (optional)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#413-download-and-install-drivers">4.1.3 Download and install drivers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#a-example-mellanox-enabling-roce-v2-rdma-optional">A. Example: Mellanox Enabling RoCE V2 RDMA (Optional)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#b-enable-pfc-priority-flow-control-to-guarantee-stable-performance-optional">B. Enable PFC (Priority Flow Control) to guarantee stable performance (optional)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#414-check-rdma-module">4.1.4 Check RDMA module</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#5-install-dependencies-for-pmem-shuffle">5. Install dependencies for PMem Shuffle</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#51-install-hpnl-httpsgithubcomintel-bigdatahpnl">5.1 Install HPNL (https://github.com/Intel-bigdata/HPNL)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#511-build-and-install-hpnl">5.1.1 Build and install HPNL</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#52-install-basic-c-library-dependencies">5.2 install basic C library dependencies</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#53-install-ndctl">5.3 install ndctl</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#54-install-pmdk">5.4 install PMDK</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#55-install-rpmem-extension">5.5 Install RPMem extension</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#6-install-pmem-shuffle-for-spark">6. Install PMem Shuffle for Spark</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#61-configure-rpmem-extension-for-spark-shuffle-in-spark">6.1 Configure RPMem extension for spark shuffle in Spark</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#prerequisite">Prerequisite</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#switch-onoff-pmem-and-rdma">Switch On/Off PMem and RDMA</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#add-pmem-information-to-spark-config">Add PMem information to spark config</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#memory-configuration-suggestion">Memory configuration suggestion</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#7-pmem-shuffle-for-spark-testing">7. PMem Shuffle for Spark Testing</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#71-decision-support-workloads">7.1 Decision support workloads</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#711-download-spark-sql-perf">7.1.1 Download spark-sql-perf</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#712-download-the-kit">7.1.2 Download the kit</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#713-prepare-data">7.1.3 Prepare data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#714-run-the-benchmark">7.1.4 Run the benchmark</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#715-check-the-result">7.1.5 Check the result</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#72-terasort">7.2 TeraSort</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#721-download-hibench">7.2.1 Download HiBench</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#722-build-hibench-as-per-instructions-from-build-bench">7.2.2 Build HiBench as per instructions from build-bench.</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#723-configuration">7.2.3 Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#724-launch-the-benchmark">7.2.4 Launch the benchmark</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#725-check-the-result">7.2.5 Check the result</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#reference">Reference</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#rpmemshuffle-spark-configuration">RPMemShuffle Spark configuration</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#8-trouble-shooting">8. Trouble shooting</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#reference-guides-without-bkc-access">Reference guides (without BKC access)</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../OAP-Installation-Guide/">OAP Installation Guide</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../OAP-Developer-Guide/">OAP Developer Guide</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="" href="../../">Version Selector</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">PMem Shuffle - 1.1.1</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>User Guide</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="pmem-shuffle-for-apache-spark-guide">PMem Shuffle for Apache Spark Guide</h1>
<p><a href="#pmem-shuffle-introduction">1. PMem Shuffle introduction</a><br />
<a href="#recommended-hw-environment">2. Recommended HW environment</a><br />
<a href="#install-and-configure-pmem">3. Install and configure PMem</a><br />
<a href="#configure-and-validate-rdma">4. Configure and Validate RDMA</a><br />
<a href="#install-dependencies-for-pmem-shuffle">5. Install dependencies for PMem Shuffle</a><br />
<a href="#install-pmem-shuffle-for-spark">6. Install PMem Shuffle for Spark</a><br />
<a href="#pmem-shuffle-for-spark-testing">7. PMem Shuffle for Spark Testing</a><br />
<a href="#trouble-shooting">8. Trouble Shooting</a><br />
<a href="#reference">Reference</a>   </p>
<p>PMem Shuffle for Spark (previously Spark-PMoF) depends on multiple
native libraries like libfabrics, libcuckoo, PMDK. This enabling guide
covers the installing process for the time being, but it might change as
the install commands and related dependency packages for the 3rd party
libraries might vary depending on the OS version and distribution you are
using.
Yarn, HDFS, Spark installation and configuration is out of the scope of this document.</p>
<h2 id="1-pmem-shuffle-introduction"><a id="pmem-shuffle-introduction"></a>1. PMem Shuffle introduction</h2>
<p>Intel Optane DC persistent memory is the next-generation storage
at memory speed. It closes the performance gap between DRAM memory
technology and traditional NAND SSDs. Remote Persistent Memory extends PMem usage to new
scenario, lots of new usage cases &amp; value proposition can be developed.</p>
<p>Spark shuffle is a high cost operation as it issues a great number of
small random disk IO, serialization, network data transmission, and thus
contributes a lot to job latency and could be the bottleneck for
workloads performance.</p>
<p>PMem Shuffle for spark (previously Spark PMoF)
<a href="https://github.com/Intel-bigdata/Spark-PMoF">https://github.com/Intel-bigdata/Spark-PMoF</a>) is a Persistent Memory
over Fabrics (PMoF) plugin for Spark shuffle, which leverages the RDMA
network and remote persistent memory (for read) to provide extremely
high performance and low latency shuffle solutions for Spark to address performance issues for shuffle intensive workloads. </p>
<p>PMem Shuffle brings follow benefits:</p>
<ul>
<li>Leverage high performance persistent memory as shuffle media as well
    as spill media, increased shuffle performance and reduced memory
    footprint</li>
<li>Using PMDK libs to avoid inefficient context switches and memory
    copies with zero-copy remote access to persistent memory.</li>
<li>Leveraging RDMA for network offloading</li>
</ul>
<p>The Figure 1 shows the high level architecture of PMem Shuffle, it shows how data flows between Spark and shuffle devices in
PMem Shuffle for spark shuffle and Vanilla Spark. In this guide, we
will introduce how to deploy and use PMem Shuffle for Spark.</p>
<p><img alt="architecture" src="../image/RPMem_shuffle_architecture.png" /></p>
<p>Figure 1: PMem Shuffle for Spark</p>
<h2 id="2-recommended-hw-environment"><a id="recommended-hw-environment"></a>2. Recommended HW environment</h2>
<h3 id="21-system-configuration">2.1. System Configuration</h3>
<hr />
<h4 id="211-hw-and-sw-configuration">2.1.1 HW and SW Configuration</h4>
<hr />
<p>A 4x or 3x Node cluster is recommended for a proof of concept tests, depending your
system configurations, if using 3 nodes cluster, the Name node and Spark
Master node can be co-located with one of the Hadoop data nodes.</p>
<p><strong>Hardware:</strong>
-   Intel® Xeon™ processor Gold 6240 CPU @ 2.60GHz, 384GB Memory (12x
    32GB 2666 MT/s) or 192GB Memory (12x 16GB 2666MT/s)
-   An RDMA capable NIC, 40Gb+ is preferred. e.g., 1x Intel X722 NIC or
    Mellanox ConnectX-4 40Gb NIC
    -   RDMA cables:
    -   Mellanox MCP1600-C003 100GbE 3m 28AWG
-   Shuffle Devices：
    -   1x 1TB HDD for shuffle (baseline)
    -   4x 128GB Persistent Memory for shuffle
-   4x 1T NVMe for HDFS
<strong>Switch</strong>:
    -   Arista 7060 CX2 (7060CX2-32S-F) 100Gb switches was used</p>
<ul>
<li>Please refer to section 4.2 for configurations
<strong>Software:</strong></li>
<li>Hadoop 2.7</li>
<li>Spark 3.1.1</li>
<li>Fedora 29 with ww08.2019 BKC</li>
</ul>
<h3 id="22-recommended-rdma-nic">2.2. Recommended RDMA NIC</h3>
<hr />
<p>PMem Shuffle is using HPNL
(<a href="https://cloud.google.com/solutions/big-data/">https://cloud.google.com/solutions/big-data/</a>) for network
communication, which leverages libfabric for efficient network
communication, so a RDMA capable NIC is recommended. Libfabric supports
RoCE, iWrap, IB protocol, so various RNICs with different protocol can
be used.</p>
<h3 id="23-recommended-pmem-configuration">2.3 Recommended PMEM configuration</h3>
<hr />
<p>It is recommended to install 4+ PMem DIMMs on the SUT, but you can
adjust the numbers accordingly. In this enabling guide, 4x 128GB PMEMM
was installed on the SUT as an exmaple. </p>
<h3 id="24-recommended-pmem-bkc-optional">2.4 Recommended PMEM BKC (optional)</h3>
<hr />
<p>This development guide was based on ww08.2019 BKC (best known configuration). Please contact your HW vendor for latest BKC.</p>
<p>Please refer to backup if you do not have BKC access. BKC
installation/enabling or FW installation is out of the scope of this guide.</p>
<h2 id="3-install-and-configure-pmem-example"><a id="install-and-configure-pmem"></a>3. Install and configure PMEM (example)</h2>
<p>1)  Please install <em>ipmctl</em> and <em>ndctl</em> according to your OS version
2)  Run <em>ipmctl show -dimm</em> to check whether dimms can be recognized
3)  Run <em>ipmctl create -goal PersistentMemoryType=AppDirect</em> to create AD
    mode
4)  Run <em>ndctl list -R</em>, you will see <strong>region0</strong> and <strong>region1</strong>. 
5)  Assume you have 4x PMEM installed on 1 node.<br />
    a.  Run <em>ndctl create-namespace -m devdax -r region0 -s 120g</em><br />
    b.  Run <em>ndctl create-namespace -m devdax -r region0 -s 120g</em><br />
    c.  Run <em>ndctl create-namespace -m devdax -r region1 -s 120g</em><br />
    d.  Run <em>ndctl create-namespace -m devdax -r region1 -s 120g</em><br />
    This will create four namespaces, namely /dev/dax0.0, /dev/dax0.1, /dev/dax1.0,
        /dev/dax1.1 in that node, and it will be used as PMem Shuffle media. </p>
<pre><code>    You can change your configuration (namespaces numbers, size) accordingly.
</code></pre>
<p>6)  Step 5 is required only when running this solution over RDMA is considered.  Otherwise PMem can be initialized in fsdax mode.<br />
    a.  Run <em>ndctl create-namespace -m fsdax -r region0 -s 120g</em><br />
    b.  Run <em>ndctl create-namespace -m fsdax -r region0 -s 120g</em><br />
    c.  Run <em>ndctl create-namespace -m fsdax -r region1 -s 120g</em><br />
    d.  Run <em>ndctl create-namespace -m fsdax -r region1 -s 120g</em><br />
    Four namespaces /dev/pmem0, /dev/pmem0.1, /dev/pmem1, /dev/pmem1.1 are created. Note that the namespace name might vary due to existing namespaces. In general, the name is consistent with the pattern /dev/pmem*.     </p>
<pre><code>After creating the namespace in fsdax mode, the namespace is ready for a file system. Here we use Ext4 file system in enabling. 
e.  mkfs.ext4 /dev/pmem0  
f.  mkfs.ext4 /dev/pmem0.1  
g.  mkfs.ext4 /dev/pmem1  
h.  mkfs.ext4 /dev/pmem1.1

Create directories and mount file system to them. To get the DAX functionality, mount the file system with dax option.

i.  mkdir /mnt/pmem0 &amp;&amp; mount -o dax /dev/pmem0 /mnt/pmem0  
j.  mkdir /mnt/pmem0.1 &amp;&amp; mount -o dax /dev/pmem0.1 /mnt/pmem0.1  
k.  mkdir /mnt/pmem1 &amp;&amp; mount -o dax /dev/pmem1 /mnt/pmem1  
l.  mkdir /mnt/pmem1.1 &amp;&amp; mount -o dax /dev/pmem1.1 /mnt/pmem1.1

Change configurations accordingly.
</code></pre>
<h2 id="4-configure-and-validate-rdma"><a id="configure-and-validate-rdma"></a>4. Configure and Validate RDMA</h2>
<hr />
<p><strong>Notes</strong>
This part is vendor specific, it might NOT apply to your environment, please check your switch, NIC manuals accordingly. </p>
<h3 id="41-configure-and-test-iwarp-rdma">4.1 Configure and test iWARP RDMA</h3>
<hr />
<h4 id="411-download-rdma-core-and-install-dependencies">4.1.1 Download rdma-core and install dependencies</h4>
<p>The rdma-core provides the necessary <strong>userspace libraries</strong> to test
rdma connectivity with tests such as rping. Refer to latest rdma-core
documentation for updated installation guidelines (https://github.com/linux-rdma/rdma-core.git).</p>
<p>You might refer to HW specific instructions or guide to enable your
RDMA NICs. Take Mellanox as an example, perform below steps to enable
it:</p>
<pre><code class="bash">git clone &lt;https://github.com/linux-rdma/rdma-core.git&gt;
dnf install cmake gcc libnl3-devel libudev-devel pkgconfig
    valgrind-devel ninja-build python3-devel python3-Cython
    python3-docutils pandoc
 //change to yum on centos
 bash build.sh
 #on centos 7
 yum install cmake gcc libnl3-devel libudev-devel make pkgconfig
    valgrind-devel
 yum install epel-release
 yum install cmake3 ninja-build pandoc
</code></pre>

<h4 id="412-switch-configuration-optional">4.1.2 Switch Configuration (optional)</h4>
<p>This part is HW specific, <strong>please check your switch manual accordingly.</strong> 
Connect the console port to PC. Username is admin. No password. Enter
global configuration mode.</p>
<p>Below example is based on Arista 7060 CX2 100Gb Switch, it is to configure the
100Gb port to work at 40Gb to match the NIC speed. <em>It is NOT required if your NIC and calbes are match.</em></p>
<p><strong>Config Switch Speed to 40Gb/s</strong></p>
<pre><code>switch# enable
switch# config
switch(config)# show interface status
</code></pre>

<p><strong>Configure corresponding port to 40 Gb/s to match the NIC speed</strong></p>
<pre><code>switch(config)# interface Et(num_of_port)/1
switch(config)# speed forced 40gfull
</code></pre>

<p>RoCE might have performance issues, so PFC configuration is strongly
suggested. You will need to check the RDMA NIC driver manual and switch
manual to configure PFC. Below is the example for ConnectX-4 and Arista
7060-CX2 switches.</p>
<p>Below is to set the two connection ports in the same vlan and
configure it in trunk mode.</p>
<p><strong>Configure interface as trunk mode and add to vlan</strong></p>
<pre><code class="bash">switch(config)# vlan 1
switch(config-vlan-1)#
switch(config)# interface ethernet 12-16
switch(config-if-Et12-16)# switchport trunk allowed vlan 1
switch (config-if-et1) # **priority-flow-control on**
switch (config-if-et1) # **priority-flow-control priority 3 no-drop**
</code></pre>

<h4 id="413-download-and-install-drivers">4.1.3 Download and install drivers</h4>
<h4 id="a-example-mellanox-enabling-roce-v2-rdma-optional">A. Example: Mellanox Enabling RoCE V2 RDMA (Optional)</h4>
<p>There are lots of packages need to be installed for dependency, please refer to your RDMA NIC's manualls to install it correctly. </p>
<pre><code class="bash">yum install atk gcc-gfortran tcsh gtk2 tcl tk
</code></pre>

<p>please install NIC drivers accordingly.</p>
<pre><code class="bash"># Download MLNX_OFED_LINUX-4.7-3.2.9.0-* from https://community.mellanox.com/s/article/howto-install-mlnx-ofed-driver 
# e.g., wget http://www.mellanox.com/downloads/ofed/MLNX_OFED-&lt;version&gt;/MLNX_OFED_LINUX-&lt;version&gt;-&lt;distribution&gt;-&lt;arch&gt;.tgz . 
tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-*
cd MLNX_OFED_LINUX-4.7-3.2.9.0-
./mlnxofedinstall --add-kernel-support.
# The process might interpret and promote you to install dependencies. Install dependencies and try again
# This process will take some time. 

</code></pre>

<pre><code class="bash">tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-*
cd MLNX_OFED_LINUX-4.7-3.2.9.0-
./mlnxofedinstall --add-kernel-support.
# The process might interpret and promote you to install
    dependencies. Install dependencies and try again
# This process will take some time. **
</code></pre>

<p>Restart the driver:</p>
<pre><code class="bash">/etc/init.d/openibd restart
</code></pre>

<p>Might need to unload the modules if it is in use.
Make sure the that the field link_layer is “Ethernet”. 
Then you can use following command to get the device name.</p>
<h4 id="b-enable-pfc-priority-flow-control-to-guarantee-stable-performance-optional">B. Enable PFC (Priority Flow Control) to guarantee stable performance (optional)</h4>
<p>Then you can use following command to get the device name</p>
<p>If you’re using Mellanox NIC, PFC is a must to guarantee stable
performance.</p>
<p>Fetch RDMA info with rdma command:</p>
<pre><code class="bash">rdma link
0/1: i40iw0/1: state DOWN physical_state NOP
1/1: i40iw1/1: state ACTIVE physical_state NOP
2/1: mlx5_0/1: state DOWN physical_state DISABLED netdev ens803f0
3/1: mlx5_1/1: state ACTIVE physical_state LINK_UP netdev ens803f1

lspci | grep Mellanox
86:00.0 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4]
86:00.1 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4]

</code></pre>

<p>Set PFC: </p>
<pre><code class="bash">/etc/init.d/openibd restart
mlnx_qos -i ens803f1 --pfc 0,0,0,1,0,0,0,0
modprobe 8021q
vconfig add ens803f1  100
ifconfig ens803f1.100 $ip1/$mask up //change to your own IP 
ifconfig ens803f1 $ip2/$mask up //Change to your own IP 
for i in {0..7}; do vconfig set_egress_map ens803f1.100 $i 3 ; done
tc_wrap.py -i ens803f1 -u 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
</code></pre>

<p>Modify the IP address part based on your environment and execute the
script.</p>
<h4 id="414-check-rdma-module">4.1.4 Check RDMA module</h4>
<p>Make sure the following modules are loaded:</p>
<pre><code class="bash">modprobe ib_core i40iw iw_cm rdma_cm rdma_ucm ib_cm ib_uverbs
 ```
#### 4.1.5 Validate RDMA functionalities  

Check that you see your RDMA interfaces listed on each server when you
run the following command: **ibv_devices**

Check with rping for RDMA connectivity between target interface and
client interface. 

 1) Assign IPs to the RDMA interfaces on Target and Client.
 2) On Target run:rping -sdVa &amp;lt;Target IP&amp;gt;
 3) On Client run: rping -cdVa &amp;lt;Target IP&amp;gt;

Example:

On the server side:
```bash 
rping -sda $ip1
created cm_id 0x17766d0
rdma_bind_addr successful
rdma_listen

accepting client connection request
cq_thread started.
recv completion
Received rkey 97a4f addr 17ce190 len 64 from peer
cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90
(child)
ESTABLISHED
Received rkey 96b40 addr 17ce1e0 len 64 from peer
server received sink adv
rdma write from lkey 143c0 laddr 1771190 len 64
rdma write completion
rping -sda $ip2
created cm_id 0x17766d0
rdma_bind_addr successful
rdma_listen
…

accepting client connection request
cq_thread started.
recv completion
Received rkey 97a4f addr 17ce190 len 64 from peer
cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90
(child)
ESTABLISHED
…
Received rkey 96b40 addr 17ce1e0 len 64 from peer
server received sink adv
rdma write from lkey 143c0 laddr 1771190 len 64
rdma write completion
…
 ```
On Client run: rping -cdVa &amp;lt;Target IP&amp;gt;

```bash 
# Client side use .100 ip 172.168.0.209 for an example 
rping -c -a 172.168.0.209 -v -C 4
ping data: rdma-ping-0: ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqr
ping data: rdma-ping-1: BCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrs
ping data: rdma-ping-2: CDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrst
ping data: rdma-ping-3: DEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstu
</code></pre>

<p>Please refer to your NIC manuual for detail instructions on how to validate RDMA works. </p>
<h2 id="5-install-dependencies-for-pmem-shuffle"><a id="install-dependencies-for-pmem-shuffle"></a>5. Install dependencies for PMem Shuffle</h2>
<hr />
<p>We have provided a Conda package which will automatically install dependencies needed for PMem Shuffle, refer to <a href="../OAP-Installation-Guide/">OAP-Installation-Guide</a> for more information. If you have finished <a href="../OAP-Installation-Guide/">OAP-Installation-Guide</a>, you can find compiled OAP jars in <code>$HOME/miniconda2/envs/oapenv/oap_jars/</code>,  and skip this session and jump to <a href="#install-pmem-shuffle-for-spark">6.Install PMem Shuffle for Spark</a></p>
<h3 id="51-install-hpnl-httpsgithubcomintel-bigdatahpnl">5.1 Install HPNL (<a href="https://github.com/Intel-bigdata/HPNL">https://github.com/Intel-bigdata/HPNL</a>)</h3>
<hr />
<p>HPNL is a fast, CPU-Efficient network library designed for modern
network technology. HPNL depends on Libfabric, which is protocol
independent, it supports TCP/IP, RoCE, IB, iWRAP etc. Please make sure
the Libfabric is installed in your setup. Based on this
<a href="https://github.com/ofiwg/libfabric/issues/5548">issue</a>, please make sure NOT
to install Libfabric 1.9.0.</p>
<p>You might need to install automake/libtool first to resolve dependency
issues.</p>
<pre><code class="bash">git clone https://github.com/ofiwg/libfabric.git
cd  libfabric
git checkout v1.6.0
./autogen.sh
./configure --disable-sockets --enable-verbs --disable-mlx
make -j &amp;&amp; sudo make install
</code></pre>

<h4 id="511-build-and-install-hpnl">5.1.1 Build and install HPNL</h4>
<p>Assume <em>Project_root_path</em> is HPNL folder’s path, <em>HPNL</em> here. </p>
<pre><code class="bash">sudo apt-get install cmake libboost-dev libboost-system-dev

#Fedora
dnf install cmake boost-devel boost-system
git clone https://github.com/Intel-bigdata/HPNL.git
cd HPNL
git checkout origin/spark-pmof-test --track
git submodule update --init --recursive
mkdir build; cd build
cmake -DWITH_VERBS=ON ..
make -j &amp;&amp; make install
cd ${project_root_path}/java/hpnl
mvn install
</code></pre>

<h3 id="52-install-basic-c-library-dependencies">5.2 install basic C library dependencies</h3>
<hr />
<pre><code class="bash">yum install -y autoconf asciidoctor kmod-devel.x86\_64 libudev-devel libuuid-devel json-c-devel jemalloc-devel
yum groupinstall -y &quot;Development Tools&quot;
</code></pre>

<h3 id="53-install-ndctl">5.3 install ndctl</h3>
<hr />
<p>This can be installed with your package managmenet tool as well. </p>
<pre><code class="bash">git clone https://github.com/pmem/ndctl.git
cd ndctl
git checkout v63
./autogen.sh
./configure CFLAGS='-g -O2' --prefix=/usr --sysconfdir=/etc
    --libdir=/usr/lib64
make -j
make check
make install
</code></pre>

<h3 id="54-install-pmdk">5.4 install PMDK</h3>
<hr />
<pre><code class="bash">yum install -y pandoc
git clone https://github.com/pmem/pmdk.git
cd pmdk
git checkout tags/1.8
make -j &amp;&amp; make install
export PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH
echo “export PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH” &gt; /etc/profile.d/pmdk.sh

</code></pre>

<h3 id="55-install-rpmem-extension">5.5 Install RPMem extension</h3>
<hr />
<pre><code class="bash">git clone  https://github.com/efficient/libcuckoo
cd libcuckoo
mkdir build
cd build
cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DBUILD_EXAMPLES=1 -DBUILD_TESTS=1 ..
make all &amp;&amp; make install
git clone -b &lt;tag-version&gt; https://github.com/intel-bigdata/OAP.git
cd OAP/oap-shuffle/RPMem-shuffle
mvn install -DskipTests

</code></pre>

<h2 id="6-install-pmem-shuffle-for-spark"><a id="install-pmem-shuffle-for-spark"></a>6. Install PMem Shuffle for Spark</h2>
<hr />
<h3 id="61-configure-rpmem-extension-for-spark-shuffle-in-spark">6.1 Configure RPMem extension for spark shuffle in Spark</h3>
<hr />
<p>PMem Shuffle for spark shuffle is designed as a plugin to Spark.
Currently the plugin supports Spark 3.1.1 and works well on various
Network fabrics, including Socket, RDMA and Omni-Path. There are several
configurations files needs to be modified in order to run PMem Shuffle. </p>
<h4 id="prerequisite">Prerequisite</h4>
<p>Use below command to remove original initialization of one PMem, this is a
<strong>MUST</strong> step, or RPMemShuffle won’t be able to open PMem devices.</p>
<pre><code class="bash">pmempool rm ${device_name}
#example: pmempool rm /dev/dax0.0
</code></pre>

<p>If you install <a href="https://anaconda.org/intel/oap">OAP Conda package</a>, you can use below command to  remove original initialization of one PMem.
```shell script
export LD_LIBRARY_PATH=$HOME/miniconda2/envs/oapenv/lib/:$LD_LIBRARY_PATH
$HOME/miniconda2/envs/oapenv/bin/pmempool rm ${device_name}</p>
<pre><code>

**Refer to the Reference section for detail descrption of each parameter.** 

#### Enable RPMemShuffle
```bash
spark.shuffle.manager             org.apache.spark.shuffle.pmof.PmofShuffleManager
spark.driver.extraClassPath           /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-&lt;version&gt;-jar-with-dependencies.jar
spark.executor.extraClassPath         /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-&lt;version&gt;-jar-with-dependencies.jar

</code></pre>

<h4 id="switch-onoff-pmem-and-rdma">Switch On/Off PMem and RDMA</h4>
<pre><code class="bash">spark.shuffle.pmof.enable_rdma                  true
spark.shuffle.pmof.enable_pmem                  true
</code></pre>

<h4 id="add-pmem-information-to-spark-config">Add PMem information to spark config</h4>
<p>Explanation: 
spark.shuffle.pmof.pmem_capacity: the capacity of one PMem device, this
value will be used when register PMem device to RDMA. </p>
<p>spark.shuffle.pmof.pmem_list: a list of all local PMem device, make
sure your per physical node executor number won’t exceed PMem device
number, or one PMem device maybe opened by two spark executor processes
and this will leads to a PMem open failure.</p>
<p>spark.shuffle.pmof.dev_core_set: a mapping of which core range will be
task set to which PMem device, this is a performance optimal
configuration for better PMem numa accessing.</p>
<p>spark.io.compression.codec: use “snappy” to do shuffle data and spilling
data compression, this is a <strong>MUST</strong> when enabled PMem due to a default LZ4 ByteBuffer incompatible issue.</p>
<pre><code class="bash">spark.shuffle.pmof.pmem_capacity                    ${total_size_of_one_device}
spark.shuffle.pmof.pmem_list                        ${device_name},${device_name},…
spark.shuffle.pmof.dev_core_set                     ${device_name}:${core_range};…
#example: 
/dev/dax0.0:0-17,36-53;/dev/dax0.2:0-17,36-53
spark.io.compression.codec                              snappy

</code></pre>

<h4 id="memory-configuration-suggestion">Memory configuration suggestion</h4>
<p>Suitable for any release before OAP 0.8. In OAP 0.8 and later release, the memory footprint of each core is reduced dramatically and the formula below is not applicable any more.  </p>
<p>Spark.executor.memory must be greater than shuffle_block_size *
numPartitions * numCores * 2 (for both shuffle and external sort), for example, default HiBench Terasort
numPartition is 200, and we configured 10 cores each executor, then this
executor must has memory capacity greater than
2MB(spark.shuffle.pmof.shuffle_block_size) * 200 * 10 * 2 = 8G. </p>
<p>Recommendation configuration as below, but it needs to be adjusted accordingly based on your system configurations. </p>
<pre><code class="bash">Yarn.executor.num    4                                          // same as PMem namespaces number
Yarn.executor.cores  18                                         // total core number divide executor number
spark.executor.memory  15g                                      // 2MB * numPartition(200) * 18 * 2
spark.yarn.executor.memoryOverhead 5g                           // 30% of  spark.executor.memory
spark.shuffle.pmof.shuffle_block_size   2096128             // 2MB – 1024 Bytes
spark.shuffle.pmof.spill_throttle       2096128                 // 2MB – 1024 Bytes, spill_throttle is used to
                                                                // set throttle by when spill buffer data to
                                // Persistent Memory, must set spill_throttle
                                // equal to shuffle_block_size
spark.driver.memory                         10g
spark.yarn.driver.memoryOverhead                5g


</code></pre>

<p><strong>Configuration of RDMA enabled case</strong></p>
<p>spark.shuffle.pmof.node : spark nodes and RDMA ip mapping list<br />
spark.driver.rhost / spark.driver.rport : Specify spark driver RDMA IP and port</p>
<pre><code class="bash">spark.shuffle.pmof.server_buffer_nums                       64
spark.shuffle.pmof.client_buffer_nums                       64
spark.shuffle.pmof.map_serializer_buffer_size           262144
spark.shuffle.pmof.reduce_serializer_buffer_size        262144
spark.shuffle.pmof.chunk_size                                   262144
spark.shuffle.pmof.server_pool_size                         3
spark.shuffle.pmof.client_pool_size                         3
spark.shuffle.pmof.node                                         $HOST1-$IP1,$HOST2-$IP2//Host-IP pairs, $hostname-$ip
spark.driver.rhost                                              $IP //change to your host IP 
spark.driver.rport                                              61000

</code></pre>

<p><strong>FSDAX</strong>
Use <code>spark.shuffle.pmof.pmpool_size</code> to specify the size of created shuffle file. The size should obey the rule: <code>max(spark.shuffle.pmof.pmpool_size) &lt; size_of_namespace * 0.9</code>. It's because we need to reserve some space in fsdax namespace for meta data. </p>
<p><strong>Misc</strong><br />
The config <code>spark.sql.shuffle.partitions</code> is required to set explicitly, it's suggested to  use default value <code>200</code> unless you're pretty sure what's the meaning of this value. </p>
<h2 id="7-pmem-shuffle-for-spark-testing"><a id="pmem-shuffle-for-spark-testing"></a>7. PMem Shuffle for Spark Testing</h2>
<hr />
<p>Pmem shuffle extension have been tested and validated with Terasort and Decision support workloads. </p>
<h3 id="71-decision-support-workloads">7.1 Decision support workloads</h3>
<hr />
<p>The  Decision support workloads is a decision support benchmark that
models several general applicable aspects of a decision support system,
including queries and data maintenance.</p>
<h4 id="711-download-spark-sql-perf">7.1.1 Download spark-sql-perf</h4>
<p>The link is <a href="https://github.com/databricks/spark-sql-perf">https://github.com/databricks/spark-sql-perf</a> and follow
README to use sbt build the artifact.</p>
<h4 id="712-download-the-kit">7.1.2 Download the kit</h4>
<p>As per instruction from spark-sql-perf README, tpcds-kit is required and
please download it from <a href="https://github.com/databricks/tpcds-kit">https://github.com/databricks/tpcds-kit</a>,
follow README to setup the benchmark.</p>
<h4 id="713-prepare-data">7.1.3 Prepare data</h4>
<p>As an example, generate parquet format data to HDFS with 1TB data scale.
The data stored path, data format and data scale are configurable.
Please check script below as a sample.</p>
<pre><code class="scala">import com.databricks.spark.sql.perf.tpcds.TPCDSTables
import org.apache.spark.sql._
// Set:
val rootDir: String = &quot;hdfs://${ip}:9000/tpcds_1T&quot;      // root directory of location to create data in.
val databaseName: String = &quot;tpcds_1T&quot;               // name of database to create.
val scaleFactor: String = &quot;1024&quot;                // scaleFactor defines the size of the dataset to generate (in GB).
val format: String = &quot;parquet&quot;                  // valid spark format like parquet &quot;parquet&quot;.
val sqlContext = new SQLContext(sc)
// Run:
val tables = new TPCDSTables(sqlContext, dsdgenDir = &quot;/mnt/spark-pmof/tool/tpcds-kit/tools&quot;, // location of dsdgen
scaleFactor = scaleFactor,
useDoubleForDecimal = false,                    // true to replace DecimalType with DoubleType
useStringForDate = false)                   // true to replace DateType with  StringType
  tables.genData(
    location = rootDir,
    format = format,
    overwrite = true,                       // overwrite the data that is already there
    partitionTables = true,                     // create the partitioned fact tables
    clusterByPartitionColumns = true,               // shuffle to get partitions coalesced into single files.
    filterOutNullPartitionValues = false,           // true to filter out the partition with NULL key value
    tableFilter = &quot;&quot;,                       // &quot;&quot; means generate all tables
    numPartitions = 400)                    // how many dsdgen partitions to run - number of input tasks.
// Create the specified database
sql(s&quot;create database $databaseName&quot;)
// Create metastore tables in a specified database for your data.
// Once tables are created, the current database will be switched to the specified database.

tables.createExternalTables(rootDir, &quot;parquet&quot;, databaseName, overwrite = true, discoverPartitions = true)

</code></pre>

<h4 id="714-run-the-benchmark">7.1.4 Run the benchmark</h4>
<p>Launch DECISION SUPPORT WORKLOADS queries on generated data, check
<em>benchmark.scala</em> below as a sample, it runs query64.</p>
<pre><code class="scala">import com.databricks.spark.sql.perf.tpcds.TPCDS
import org.apache.spark.sql._
val sqlContext = new SQLContext(sc)
val tpcds = new TPCDS (sqlContext = sqlContext)
// Set:
val databaseName = &quot;tpcds_1T&quot;                   // name of database with TPCDS data.
val resultLocation = &quot;tpcds_1T_result&quot;              // place to write results
val iterations = 1                      // how many iterations of queries to run.
val query_filter = Seq(&quot;q64-v2.4&quot;)
val randomizeQueries = false
def queries = {
val filtered_queries = query_filter match {
case Seq() =&gt; tpcds.tpcds2_4Queries
case _=&gt;  tpcds.tpcds2_4Queries.filter(q =&gt;
    query_filter.contains(q.name))
 }
filtered_queries
}
val timeout = 24*60*60                      // timeout, in seconds.
// Run:
sql(s&quot;use $databaseName&quot;)
val experiment = tpcds.runExperiment(
queries,
iterations = iterations,
resultLocation = resultLocation,
forkThread = true)
experiment.waitForFinish(timeout)
</code></pre>

<h4 id="715-check-the-result">7.1.5 Check the result</h4>
<p>Check the result under <em>tpcds_1T_result</em> folder. It can be an option
to check the result at spark history server. (Need to start history server by
<em>\$SPARK_HOME/sbin/start-history-server.sh</em>)</p>
<h3 id="72-terasort">7.2 TeraSort</h3>
<hr />
<p>TeraSort is a benchmark that measures the amount of time to sort one
terabyte of randomly distributed data on a given computer system.</p>
<h4 id="721-download-hibench">7.2.1 Download HiBench</h4>
<p>This guide uses HiBench for Terasort tests, <a href="https://github.com/Intel-bigdata/HiBench">https://github.com/Intel-bigdata/HiBench</a>. HiBench is a
big data benchmark suite and contains a set of Hadoop, Spark and
streaming workloads including TeraSort.</p>
<h4 id="722-build-hibench-as-per-instructions-from-build-bench">7.2.2 Build HiBench as per instructions from <a href="https://github.com/Intel-bigdata/HiBench/blob/master/docs/build-hibench.md">build-bench</a>.</h4>
<h4 id="723-configuration">7.2.3 Configuration</h4>
<p>Modify <em>\$HiBench-HOME/conf/spark.conf</em> to specify the spark home and
other spark configurations. It will overwrite the configuration of
<em>\$SPARK-HOME/conf/spark-defaults.conf</em> at run time.</p>
<h4 id="724-launch-the-benchmark">7.2.4 Launch the benchmark</h4>
<p>Need to prepare the data with</p>
<p><em>\$HiBench-HOME/bin/workloads/micro/terasort/prepare/prepare.sh</em></p>
<p>Kick off the evaluation by
\$HiBench-HOME/bin/workloads/micro/terasort/spark/run.sh</p>
<p>Change directory to <em>\$HiBench-HOME/bin/workloads/micro/terasort/spark</em>
and launch the <em>run.sh</em>. You can add some PMEM cleaning work to make sure
it starts from empty shuffle device every test iteration. Take <em>run.sh</em>
below as a sample.</p>
<pre><code class="bash"># ***Change below command accordingly ***
ssh ${node} pmempool rm /dev/dax0.0

current_dir=`dirname &quot;$0&quot;`
current_dir=`cd &quot;$current_dir&quot;; pwd`
root_dir=${current_dir}/../../../../..
workload_config=${root_dir}/conf/workloads/micro/terasort.conf
. &quot;${root_dir}/bin/functions/load_bench_config.sh&quot;

enter_bench ScalaSparkTerasort ${workload_config} ${current_dir}
show_bannar start

rmr_hdfs $OUTPUT_HDFS || true

SIZE=`dir_size $INPUT_HDFS`
START_TIME=`timestamp`
run_spark_job com.intel.hibench.sparkbench.micro.ScalaTeraSort $INPUT_HDFS $OUTPUT_HDFS
END_TIME=`timestamp`

gen_report ${START_TIME} ${END_TIME} ${SIZE}
show_bannar finish
leave_bench

</code></pre>

<h4 id="725-check-the-result">7.2.5 Check the result</h4>
<p>Check the result at spark history server to see the execution time and
other spark metrics like spark shuffle spill status. (Need to start
history server by <em>\$SPARK_HOME/sbin/start-history-server.sh</em>)</p>
<h2 id="reference"><a id="reference"></a>Reference</h2>
<hr />
<h3 id="rpmemshuffle-spark-configuration">RPMemShuffle Spark configuration</h3>
<hr />
<p>Before running Spark workload, add following contents in <code>spark-defaults.conf</code>.</p>
<pre><code class="bash">spark.executor.instances  4                                 // same as total PMem namespace numbers of your cluster
spark.executor.cores  18                                    // total core number divide executor number
spark.executor.memory  70g                                  // 4~5G * spark.executor.cores
spark.executor.memoryOverhead 15g                           // 30% of  spark.executor.memory
spark.shuffle.pmof.shuffle_block_size   2096128             // 2MB – 1024 Bytes
spark.shuffle.pmof.spill_throttle       2096128             // 2MB – 1024 Bytes

spark.driver.memory    10g
spark.yarn.driver.memoryOverhead 5g

spark.shuffle.compress                                      true
spark.io.compression.codec                                  snappy
spark.driver.extraClassPath                                 $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-&lt;version&gt;-jar-with-dependencies.jar
spark.executor.extraClassPath                               $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-&lt;version&gt;-jar-with-dependencies.jar
spark.shuffle.manager                                       org.apache.spark.shuffle.pmof.PmofShuffleManager
spark.shuffle.pmof.enable_rdma                              true
spark.shuffle.pmof.enable_pmem                              true
spark.shuffle.pmof.pmem_capacity                            126833655808 // size should be same as pmem size
spark.shuffle.pmof.pmem_list                                /dev/dax0.0,/dev/dax0.1,/dev/dax1.0,/dev/dax1.1
spark.shuffle.pmof.dev_core_set                             dax0.0:0-71,dax0.1:0-71,dax1.0:0-71,dax1.1:0-71
spark.shuffle.pmof.server_buffer_nums                       64
spark.shuffle.pmof.client_buffer_nums                       64
spark.shuffle.pmof.map_serializer_buffer_size               262144
spark.shuffle.pmof.reduce_serializer_buffer_size            262144
spark.shuffle.pmof.chunk_size                               262144
spark.shuffle.pmof.server_pool_size                         3
spark.shuffle.pmof.client_pool_size                         3
spark.shuffle.pmof.node                                     $host1-$IP1,$host2-$IP2//HOST-IP Pair, seperate with &quot;,&quot;
spark.driver.rhost                                          $IP //change to your host
spark.driver.rport                                          61000

</code></pre>

<h2 id="8-trouble-shooting"><a id="trouble-shooting"></a>8. Trouble shooting</h2>
<p>For any reason that a previous job is failed, please empty PMem spaces before another run.<br />
It's because normal space release operation might fail to be invoked for failed jobs.  </p>
<p>For devdax, use <code>pmempool rm {devdax-namespace}</code> to reset the entire namespace.  <br />
For fsdax, use <code>rm -rf {mounted-pmem-folder}/shuffle_block*</code> to remove corresponding shuffle pool files.  </p>
<h3 id="reference-guides-without-bkc-access">Reference guides (without BKC access)</h3>
<hr />
<p>If you do not have BKC access, please following below official guide:
(1): General PMEMM support: PMEMM support
<a href="https://www.intel.com/content/www/us/en/support/products/190349/memory-and-storage/data-center-persistent-memory/intel-optane-dc-persistent-memory.html">https://www.intel.com/content/www/us/en/support/products/190349/memory-and-storage/data-center-persistent-memory/intel-optane-dc-persistent-memory.html</a></p>
<p>(2) PMEMM population rule: Module DIMM Population for Intel® Optane™ DC
Persistent Memory
<a href="https://www.intel.com/content/www/us/en/support/articles/000032932/memory-and-storage/data-center-persistent-memory.html?productId=190349&amp;localeCode=us_en">https://www.intel.com/content/www/us/en/support/articles/000032932/memory-and-storage/data-center-persistent-memory.html?productId=190349&amp;localeCode=us_en</a></p>
<p>(3) OS support requirement: Operating System OS for Intel® Optane™ DC
Persistent Memory
<a href="https://www.intel.com/content/www/us/en/support/articles/000032860/memory-and-storage/data-center-persistent-memory.html?productId=190349&amp;localeCode=us_en">https://www.intel.com/content/www/us/en/support/articles/000032860/memory-and-storage/data-center-persistent-memory.html?productId=190349&amp;localeCode=us_en</a></p>
<p>(4): Quick Start Guide: Provision Intel® Optane™ DC Persistent Memory
<a href="https://software.intel.com/en-us/articles/quick-start-guide-configure-intel-optane-dc-persistent-memory-on-linux">https://software.intel.com/en-us/articles/quick-start-guide-configure-intel-optane-dc-persistent-memory-on-linux</a></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../OAP-Installation-Guide/" class="btn btn-neutral float-right" title="OAP Installation Guide">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
        <span style="margin-left: 15px"><a href="../OAP-Installation-Guide/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
