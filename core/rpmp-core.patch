diff --git a/core/pom.xml b/core/pom.xml
index 983f724a..37a5a17e 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -43,6 +43,11 @@
             <artifactId>hpnl</artifactId>
             <version>0.5</version>
         </dependency>
+        <dependency>
+            <groupId>com.intel.rpmp</groupId>
+            <artifactId>rpmp</artifactId>
+            <version>0.1</version>
+        </dependency>
         <dependency>
             <groupId>org.xerial</groupId>
             <artifactId>sqlite-jdbc</artifactId>
diff --git a/core/src/main/scala/org/apache/spark/shuffle/pmof/BaseShuffleReader.scala b/core/src/main/scala/org/apache/spark/shuffle/pmof/BaseShuffleReader.scala
index c417bba0..8399abbf 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/pmof/BaseShuffleReader.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/pmof/BaseShuffleReader.scala
@@ -131,7 +131,7 @@ private[spark] class BaseShuffleReader[K, C](handle: BaseShuffleHandle[K, _, C],
         assert(pmofConf.enablePmem)
         // Create an ExternalSorter to sort the data.
         val sorter =
-          new PmemExternalSorter[K, C, C](context, handle, pmofConf, ordering = Some(keyOrd), serializer = dep.serializer)
+          new PmemExternalSorter[K, C, C](context, handle, pmofConf, blockManager, ordering = Some(keyOrd), serializer = dep.serializer)
         sorter.insertAllAndUpdateMetrics(aggregatedIter)
       case None =>
         aggregatedIter
diff --git a/core/src/main/scala/org/apache/spark/shuffle/pmof/MetadataResolver.scala b/core/src/main/scala/org/apache/spark/shuffle/pmof/MetadataResolver.scala
index 7b2984b4..50d96a14 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/pmof/MetadataResolver.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/pmof/MetadataResolver.scala
@@ -44,7 +44,7 @@ class MetadataResolver(pmofConf: PmofConf) {
     * @param rkey
     */
   def pushPmemBlockInfo(shuffleId: Int, mapId: Long, dataAddressMap: mutable.HashMap[Int, Array[(Long, Int)]], rkey: Long): Unit = {
-    val buffer: Array[Byte] = new Array[Byte](pmofConf.reduce_serializer_buffer_size.toInt)
+    val buffer: Array[Byte] = new Array[Byte](pmofConf.map_serializer_buffer_size.toInt)
     var output = new Output(buffer)
     val bufferArray = new ArrayBuffer[ByteBuffer]()
 
@@ -60,7 +60,7 @@ class MetadataResolver(pmofConf: PmofConf) {
             blockBuffer.flip()
             bufferArray += blockBuffer
             output.close()
-            val new_buffer = new Array[Byte](pmofConf.reduce_serializer_buffer_size.toInt)
+            val new_buffer = new Array[Byte](pmofConf.map_serializer_buffer_size.toInt)
             output = new Output(new_buffer)
           }
         }
diff --git a/core/src/main/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriter.scala b/core/src/main/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriter.scala
index 147ac27c..20727e34 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriter.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriter.scala
@@ -21,6 +21,7 @@ import org.apache.spark._
 import org.apache.spark.internal.Logging
 import org.apache.spark.network.pmof.PmofTransferService
 import org.apache.spark.scheduler.MapStatus
+import org.apache.spark.scheduler.pmof.UnCompressedMapStatus
 import org.apache.spark.shuffle.{BaseShuffleHandle, ShuffleWriter}
 import org.apache.spark.storage._
 import org.apache.spark.util.collection.pmof.PmemExternalSorter
@@ -76,7 +77,7 @@ private[spark] class PmemShuffleWriter[K, V, C](shuffleBlockResolver: PmemShuffl
 
     if (dep.mapSideCombine) { // do aggregation
       if (dep.aggregator.isDefined) {
-        sorter = new PmemExternalSorter[K, V, C](context, handle, pmofConf, dep.aggregator, Some(dep.partitioner),
+        sorter = new PmemExternalSorter[K, V, C](context, handle, pmofConf, blockManager, dep.aggregator, Some(dep.partitioner),
           dep.keyOrdering, dep.serializer)
 				sorter.setPartitionByteBufferArray(PmemBlockOutputStreamArray)
         sorter.insertAll(records)
@@ -107,12 +108,19 @@ private[spark] class PmemShuffleWriter[K, V, C](shuffleBlockResolver: PmemShuffl
     val pmemBlockInfoMap = mutable.HashMap.empty[Int, Array[(Long, Int)]]
     var output_str : String = ""
 
+    var rKey: Int = 0
     for (i <- spillPartitionArray) {
-      if (pmofConf.enableRdma) {
-        pmemBlockInfoMap(i) = PmemBlockOutputStreamArray(i).getPartitionMeta().map { info => (info._1, info._2) }
+      if (pmofConf.enableRdma && !pmofConf.enableRemotePmem) {
+        pmemBlockInfoMap(i) = PmemBlockOutputStreamArray(i).getPartitionMeta().map (info => {
+          if (rKey == 0) {
+            rKey = info._3
+          }
+          //logInfo(s"${ShuffleBlockId(stageId, mapId, i)} [${rKey}]${info._1}:${info._2}")
+          (info._1, info._2)
+        })
       }
       partitionLengths(i) = PmemBlockOutputStreamArray(i).size
-      output_str += "\tPartition " + i + ": " + partitionLengths(i) + ", records: " + PmemBlockOutputStreamArray(i).records + "\n"
+      output_str += s"\t${ShuffleBlockId(stageId, mapId, i)}: ${partitionLengths(i)}, records: ${PmemBlockOutputStreamArray(i).records}\n"
     }
 
     for (i <- 0 until numPartitions) {
@@ -120,6 +128,7 @@ private[spark] class PmemShuffleWriter[K, V, C](shuffleBlockResolver: PmemShuffl
     }
 
     val shuffleServerId = blockManager.shuffleServerId
+    /**
     if (pmofConf.enableRdma) {
       val rkey = PmemBlockOutputStreamArray(0).getRkey()
       metadataResolver.pushPmemBlockInfo(stageId, mapId, pmemBlockInfoMap, rkey)
@@ -130,6 +139,23 @@ private[spark] class PmemShuffleWriter[K, V, C](shuffleBlockResolver: PmemShuffl
     } else {
       mapStatus = MapStatus(shuffleServerId, partitionLengths, mapId)
     }
+    **/
+
+    if (pmofConf.enableRemotePmem) {
+      mapStatus = new UnCompressedMapStatus(shuffleServerId, partitionLengths, mapId)
+      //mapStatus = MapStatus(shuffleServerId, partitionLengths)
+    } else if (!pmofConf.enableRdma) {
+      mapStatus = MapStatus(shuffleServerId, partitionLengths, mapId)
+    } else {
+      metadataResolver.pushPmemBlockInfo(stageId, mapId, pmemBlockInfoMap, rKey)
+      val blockManagerId: BlockManagerId =
+        BlockManagerId(
+          shuffleServerId.executorId,
+          PmofTransferService.shuffleNodesMap(shuffleServerId.host),
+          PmofTransferService.getTransferServiceInstance(pmofConf, blockManager).port,
+          shuffleServerId.topologyInfo)
+      mapStatus = MapStatus(blockManagerId, partitionLengths, mapId)
+    }
   }
 
   /** Close this writer, passing along whether the map completed */
diff --git a/core/src/main/scala/org/apache/spark/shuffle/pmof/PmofShuffleManager.scala b/core/src/main/scala/org/apache/spark/shuffle/pmof/PmofShuffleManager.scala
index 63b8241b..51a6b3bc 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/pmof/PmofShuffleManager.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/pmof/PmofShuffleManager.scala
@@ -21,7 +21,7 @@ private[spark] class PmofShuffleManager(conf: SparkConf) extends ShuffleManager
    */
   private[this] val taskIdMapsForShuffle = new ConcurrentHashMap[Int, OpenHashSet[Long]]()
 
-  private[this] val pmofConf = new PmofConf(conf)
+  private[this] val pmofConf = PmofConf.getConf(conf)
   var metadataResolver: MetadataResolver = _
 
   override def registerShuffle[K, V, C](shuffleId: Int, dependency: ShuffleDependency[K, V, C]): ShuffleHandle = {
@@ -62,9 +62,27 @@ private[spark] class PmofShuffleManager(conf: SparkConf) extends ShuffleManager
   override def getReader[K, C](handle: _root_.org.apache.spark.shuffle.ShuffleHandle, startMapIndex: Int, endMapIndex: Int, startPartition: Int, endPartition: Int, context: _root_.org.apache.spark.TaskContext, readMetrics: ShuffleReadMetricsReporter): _root_.org.apache.spark.shuffle.ShuffleReader[K, C] = {
     val blocksByAddress = SparkEnv.get.mapOutputTracker.getMapSizesByExecutorId(
       handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition)
-    if (pmofConf.enableRdma) {
-      new RdmaShuffleReader(handle.asInstanceOf[BaseShuffleHandle[K, _, C]],
-        startMapIndex, endMapIndex, startPartition, endPartition, context, pmofConf)
+    val env: SparkEnv = SparkEnv.get 
+    if (pmofConf.enableRemotePmem) {
+      new RpmpShuffleReader(
+        handle.asInstanceOf[BaseShuffleHandle[K, _, C]],
+        startMapIndex,
+        endMapIndex,
+        startPartition,
+        endPartition,
+        context,
+        pmofConf)
+    } else if (pmofConf.enableRdma) {
+      metadataResolver = MetadataResolver.getMetadataResolver(pmofConf)
+      PmofTransferService.getTransferServiceInstance(pmofConf, env.blockManager, this)
+      new RdmaShuffleReader(
+        handle.asInstanceOf[BaseShuffleHandle[K, _, C]],
+        startMapIndex,
+        endMapIndex,
+        startPartition,
+        endPartition,
+        context,
+        pmofConf)
     } else {
       new BaseShuffleReader(
         handle.asInstanceOf[BaseShuffleHandle[K, _, C]], blocksByAddress, startPartition, endPartition, context, readMetrics, pmofConf)
diff --git a/core/src/main/scala/org/apache/spark/shuffle/pmof/RdmaShuffleReader.scala b/core/src/main/scala/org/apache/spark/shuffle/pmof/RdmaShuffleReader.scala
index c2270592..8a37a57c 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/pmof/RdmaShuffleReader.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/pmof/RdmaShuffleReader.scala
@@ -31,6 +31,7 @@ private[spark] class RdmaShuffleReader[K, C](handle: BaseShuffleHandle[K, _, C],
   private[this] val dep = handle.dependency
   private[this] val serializerInstance: SerializerInstance = dep.serializer.newInstance()
   private[this] val enable_pmem: Boolean = SparkEnv.get.conf.getBoolean("spark.shuffle.pmof.enable_pmem", defaultValue = true)
+  private[this] val enable_rpmp: Boolean = SparkEnv.get.conf.getBoolean("spark.shuffle.pmof.enable_remote_pmem", defaultValue = true)
 
   /** Read the combined key-values for this reduce task */
   override def read(): Iterator[Product2[K, C]] = {
@@ -88,8 +89,8 @@ private[spark] class RdmaShuffleReader[K, C](handle: BaseShuffleHandle[K, _, C],
     // Sort the output if there is a sort ordering defined.
     dep.keyOrdering match {
       case Some(keyOrd: Ordering[K]) =>
-        if (enable_pmem) {
-          val sorter = new PmemExternalSorter[K, C, C](context, handle, pmofConf, ordering = Some(keyOrd), serializer = dep.serializer)
+        if (enable_pmem && !enable_rpmp) {
+          val sorter = new PmemExternalSorter[K, C, C](context, handle, pmofConf, blockManager, ordering = Some(keyOrd), serializer = dep.serializer)
           sorter.insertAll(aggregatedIter)
           CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())
         } else {
diff --git a/core/src/main/scala/org/apache/spark/storage/pmof/NettyByteBufferPool.scala b/core/src/main/scala/org/apache/spark/storage/pmof/NettyByteBufferPool.scala
index 5ad49d44..e7ebb84e 100644
--- a/core/src/main/scala/org/apache/spark/storage/pmof/NettyByteBufferPool.scala
+++ b/core/src/main/scala/org/apache/spark/storage/pmof/NettyByteBufferPool.scala
@@ -40,6 +40,10 @@ object NettyByteBufferPool extends Logging {
     }
   }
 
+  def allocateNewBuffer(): ByteBuf = synchronized {
+    allocator.directBuffer()
+  }
+
   def allocateFlexibleNewBuffer(bufSize: Int): ByteBuf = synchronized {
     val byteBuf = allocator.directBuffer(65536, bufSize * 2)
     bufferMap += (byteBuf -> bufSize)
diff --git a/core/src/main/scala/org/apache/spark/storage/pmof/PmemBlockInputStream.scala b/core/src/main/scala/org/apache/spark/storage/pmof/PmemBlockInputStream.scala
index bde6ad44..ff92982c 100644
--- a/core/src/main/scala/org/apache/spark/storage/pmof/PmemBlockInputStream.scala
+++ b/core/src/main/scala/org/apache/spark/storage/pmof/PmemBlockInputStream.scala
@@ -1,36 +1,40 @@
 package org.apache.spark.storage.pmof
 
+import java.io.InputStream
 import com.esotericsoftware.kryo.KryoException
+import java.io.IOException
 import org.apache.spark.SparkEnv
 import org.apache.spark.serializer.{DeserializationStream, Serializer, SerializerInstance, SerializerManager}
 import org.apache.spark.storage.BlockId
+import org.apache.spark.util.configuration.pmof.PmofConf
+import org.apache.spark.internal.Logging
 
-class PmemBlockInputStream[K, C](pmemBlockOutputStream: PmemBlockOutputStream, serializer: Serializer) {
-  val blockId: BlockId = pmemBlockOutputStream.getBlockId()
+trait PmemBlockInputStream[K, C] {
+  def readNextItem(): (K, C)
+}
+
+class LocalPmemBlockInputStream[K, C](
+    blockId: BlockId,
+    total_records: Long,
+    serializer: Serializer)
+    extends PmemBlockInputStream[K, C] {
   val serializerManager: SerializerManager = SparkEnv.get.serializerManager
   val serInstance: SerializerInstance = serializer.newInstance()
-  val persistentMemoryWriter: PersistentMemoryHandler = PersistentMemoryHandler.getPersistentMemoryHandler
+  val persistentMemoryWriter: PersistentMemoryHandler =
+    PersistentMemoryHandler.getPersistentMemoryHandler
   var pmemInputStream: PmemInputStream = new PmemInputStream(persistentMemoryWriter, blockId.name)
   val wrappedStream = serializerManager.wrapStream(blockId, pmemInputStream)
   var inObjStream: DeserializationStream = serInstance.deserializeStream(wrappedStream)
 
-  var total_records: Long = 0
   var indexInBatch: Int = 0
   var closing: Boolean = false
 
-  loadStream()
-
-  def loadStream(): Unit = {
-    total_records = pmemBlockOutputStream.getTotalRecords()
-    indexInBatch = 0
-  }
-
   def readNextItem(): (K, C) = {
     if (closing == true) {
       close()
       return null
     }
-    try{
+    try {
       val k = inObjStream.readObject().asInstanceOf[K]
       val c = inObjStream.readObject().asInstanceOf[C]
       indexInBatch += 1
@@ -39,8 +43,7 @@ class PmemBlockInputStream[K, C](pmemBlockOutputStream: PmemBlockOutputStream, s
       }
       (k, c)
     } catch {
-      case ex: KryoException => {
-      }
+      case ex: KryoException => {}
         sys.exit(0)
     }
   }
@@ -51,3 +54,76 @@ class PmemBlockInputStream[K, C](pmemBlockOutputStream: PmemBlockOutputStream, s
     inObjStream = null
   }
 }
+
+class RemotePmemBlockInputStream[K, C](
+    blockId: BlockId,
+    mapStatus: Seq[(String, Long, Int)],
+    serializer: Serializer,
+    pmofConf: PmofConf)
+    extends PmemBlockInputStream[K, C]
+    with Logging {
+  val serializerManager: SerializerManager = SparkEnv.get.serializerManager
+  val serInstance: SerializerInstance = serializer.newInstance()
+  val remotePersistentMemoryPool =
+    RemotePersistentMemoryPool.getInstance(pmofConf.rpmpHost, pmofConf.rpmpPort)
+
+  var map_index: Int = 0
+  var num_items: Int = 0
+  var cur_num_items: Int = 0
+  var inObjStream: DeserializationStream = _
+  var buf: NioManagedBuffer = _
+  var input: InputStream = _
+
+  def loadStream(): Unit = {
+    if (buf != null) {
+      inObjStream.close()
+      input.close()
+      buf.release()
+    }
+    if (map_index == mapStatus.size) {
+      inObjStream = null
+    } else {
+      num_items = mapStatus(map_index)._3
+      buf = new NioManagedBuffer(mapStatus(map_index)._2.toInt)
+      logDebug(s"[GET started] ${mapStatus(map_index)._1}-${mapStatus(map_index)._2}")
+      var retry = 0
+      while (remotePersistentMemoryPool.get(
+               mapStatus(map_index)._1,
+               mapStatus(map_index)._2,
+               buf.nioByteBuffer) == -1) {
+        logWarning(
+          s"${mapStatus(map_index)._1}-${mapStatus(map_index)._2} RPMem get failed due to time out, try again")
+        retry += 1
+        if (retry == 4) {
+          throw new IOException(
+            s"${mapStatus(map_index)._1}-${mapStatus(map_index)._2} RPMem get failed due to time out")
+        }
+      }
+      logDebug(s"[GET Completed] ${mapStatus(map_index)._1}-${mapStatus(map_index)._2}")
+      val in = buf.createInputStream()
+      input = serializerManager.wrapStream(blockId, in)
+      inObjStream = serInstance.deserializeStream(input)
+      map_index += 1
+    }
+  }
+
+  def readNextItem(): (K, C) = {
+    try {
+      if (buf == null || cur_num_items >= num_items) {
+        loadStream()
+        cur_num_items = 0
+      }
+      if (inObjStream == null) {
+        return null
+      }
+      val k = inObjStream.readObject().asInstanceOf[K]
+      val c = inObjStream.readObject().asInstanceOf[C]
+      cur_num_items += 1
+      (k, c)
+    } catch {
+      case ex: KryoException => {}
+        sys.exit(0)
+    }
+  }
+
+}
\ No newline at end of file
diff --git a/core/src/main/scala/org/apache/spark/storage/pmof/PmemBlockOutputStream.scala b/core/src/main/scala/org/apache/spark/storage/pmof/PmemBlockOutputStream.scala
index cc70c718..eab50776 100644
--- a/core/src/main/scala/org/apache/spark/storage/pmof/PmemBlockOutputStream.scala
+++ b/core/src/main/scala/org/apache/spark/storage/pmof/PmemBlockOutputStream.scala
@@ -12,17 +12,19 @@ import org.apache.spark.util.configuration.pmof.PmofConf
 
 import scala.collection.mutable.ArrayBuffer
 
-class PmemBlockId (stageId: Int, tmpId: Int) extends ShuffleBlockId(stageId, 0, tmpId) {
-  override def name: String = "reduce_spill_" + stageId + "_" + tmpId
+class PmemBlockId (executorId: String, stageId: Int, tmpName: String, tmpId: Int) extends ShuffleBlockId(stageId, 0, tmpId) {
+  override def name: String = s"reduce_spill_${executorId}_${tmpName}_${tmpId}"
   override def isShuffle: Boolean = false
 }
 
 object PmemBlockId {
   private var tempId: Int = 0
-  def getTempBlockId(stageId: Int): PmemBlockId = synchronized {
-    val cur_tempId = tempId
-    tempId += 1
-    new PmemBlockId (stageId, cur_tempId)
+  def getTempBlockId(executorId: String, tmpName: String, stageId: Int): PmemBlockId = synchronized {
+    synchronized{
+      val cur_tempId = tempId
+      tempId += 1
+      new PmemBlockId (executorId, stageId, tmpName, cur_tempId)
+    }
   }
 }
 
@@ -45,14 +47,22 @@ private[spark] class PmemBlockOutputStream(
   var partitionMeta: Array[(Long, Int, Int)] = _
   val root_dir = Utils.getConfiguredLocalDirs(conf).toList.sortWith(_ < _)(0)
 
-  val persistentMemoryWriter: PersistentMemoryHandler = PersistentMemoryHandler.getPersistentMemoryHandler(pmofConf,
-    root_dir, pmofConf.path_list, blockId.name, pmofConf.maxPoolSize)
+  var persistentMemoryWriter: PersistentMemoryHandler = _
+  var remotePersistentMemoryPool: RemotePersistentMemoryPool = _
+  val mapStatus: ArrayBuffer[(String, Long, Int)] = ArrayBuffer[(String, Long, Int)]()
+
+  if (!pmofConf.enableRemotePmem) {
+    persistentMemoryWriter = PersistentMemoryHandler.getPersistentMemoryHandler(pmofConf,
+      root_dir, pmofConf.path_list, blockId.name, pmofConf.maxPoolSize)
+  } else {
+    remotePersistentMemoryPool = RemotePersistentMemoryPool.getInstance(pmofConf.rpmpHost, pmofConf.rpmpPort)
+  }
 
   //disable metadata updating by default
   //persistentMemoryWriter.updateShuffleMeta(blockId.name)
 
   val pmemOutputStream: PmemOutputStream = new PmemOutputStream(
-    persistentMemoryWriter, numPartitions, blockId.name, numMaps, (pmofConf.spill_throttle.toInt + 1024))
+    persistentMemoryWriter, remotePersistentMemoryPool, numPartitions, blockId.name, numMaps, (pmofConf.spill_throttle.toInt + 1024))
   val serInstance = serializer.newInstance()
   val bs = serializerManager.wrapStream(blockId, pmemOutputStream)
   var objStream: SerializationStream = serInstance.serializeStream(bs)
@@ -82,15 +92,16 @@ private[spark] class PmemBlockOutputStream(
   }
 
   def maybeSpill(force: Boolean = false): Unit = {
-    if ((pmofConf.spill_throttle != -1 && pmemOutputStream.remainingSize >= pmofConf.spill_throttle) || force == true) {
+    if ((pmofConf.spill_throttle != -1 && pmemOutputStream.bufferRemainingSize >= pmofConf.spill_throttle) || force == true) {
       val start = System.nanoTime()
       flush()
-      pmemOutputStream.doFlush()
+      //pmemOutputStream.doFlush()
       val bufSize = pmemOutputStream.flushedSize
+      mapStatus += ((pmemOutputStream.flushed_block_id, bufSize, recordsPerBlock))
       if (bufSize > 0) {
         recordsArray += recordsPerBlock
         recordsPerBlock = 0
-        size += bufSize
+        size = bufSize
 
         if (blockId.isShuffle == true) {
           val writeMetrics = taskMetrics.shuffleWriteMetrics
@@ -109,10 +120,29 @@ private[spark] class PmemBlockOutputStream(
     spilled
   }
 
+  def getPartitionBlockInfo(res_array: Array[Long]): Array[(Long, Int, Int)] = {
+    var i = -3
+    var blockInfo = Array.ofDim[(Long, Int)]((res_array.length)/3)
+    blockInfo.map{
+      x => i += 3;
+      (res_array(i), res_array(i+1).toInt, res_array(i+2).toInt)
+    }
+  }
+
   def getPartitionMeta(): Array[(Long, Int, Int)] = {
     if (partitionMeta == null) {
       var i = -1
-      partitionMeta = persistentMemoryWriter.getPartitionBlockInfo(blockId.name).map{ x=> i+=1; (x._1, x._2, recordsArray(i))}
+      partitionMeta = if (!pmofConf.enableRemotePmem) {
+        persistentMemoryWriter.getPartitionBlockInfo(blockId.name).map ( x => {
+          i += 1
+          (x._1, x._2, getRkey().toInt) 
+        })
+      } else {
+        getPartitionBlockInfo(remotePersistentMemoryPool.getMeta(blockId.name)).map( x => {
+          i += 1
+          (x._1, x._2, x._3) 
+        })
+      }
     }
     partitionMeta
   }
diff --git a/core/src/main/scala/org/apache/spark/storage/pmof/PmemOutputStream.scala b/core/src/main/scala/org/apache/spark/storage/pmof/PmemOutputStream.scala
index 08c632e9..d4345ce3 100644
--- a/core/src/main/scala/org/apache/spark/storage/pmof/PmemOutputStream.scala
+++ b/core/src/main/scala/org/apache/spark/storage/pmof/PmemOutputStream.scala
@@ -2,12 +2,14 @@ package org.apache.spark.storage.pmof
 
 import java.io.OutputStream
 import java.nio.ByteBuffer
+import java.io.IOException
 
 import io.netty.buffer.{ByteBuf, PooledByteBufAllocator}
 import org.apache.spark.internal.Logging
 
 class PmemOutputStream(
   persistentMemoryWriter: PersistentMemoryHandler,
+  remotePersistentMemoryPool: RemotePersistentMemoryPool,
   numPartitions: Int,
   blockId: String,
   numMaps: Int,
@@ -15,11 +17,14 @@ class PmemOutputStream(
   ) extends OutputStream with Logging {
   var set_clean = true
   var is_closed = false
+  var key_id = 0
 
-  val length: Int = bufferSize
+  val length: Int = 1024 * 1024 * 6
   var bufferFlushedSize: Int = 0
   var bufferRemainingSize: Int = 0
   val buf: ByteBuf = NettyByteBufferPool.allocateFlexibleNewBuffer(length)
+  var flushed_block_id: String = _
+  var cur_block_id: String = blockId
 
   override def write(bytes: Array[Byte], off: Int, len: Int): Unit = {
     buf.writeBytes(bytes, off, len)
@@ -32,13 +37,33 @@ class PmemOutputStream(
   }
 
   override def flush(): Unit = {
-  }
-
-  def doFlush(): Unit = {
     if (bufferRemainingSize > 0) {
-      val byteBuffer: ByteBuffer = buf.nioBuffer()
-      persistentMemoryWriter.setPartition(numPartitions, blockId, byteBuffer, bufferRemainingSize, set_clean)
-      logDebug(s"flush ${blockId} size ${bufferRemainingSize}")
+      if (remotePersistentMemoryPool != null) {
+        logDebug(s" [PUT Started]${cur_block_id}-${bufferRemainingSize}")
+        val byteBuffer: ByteBuffer = buf.nioBuffer()
+        var retry = 0
+        while (remotePersistentMemoryPool.put(cur_block_id, byteBuffer, bufferRemainingSize) == -1) {
+          logWarning(
+            s"${cur_block_id}-${bufferRemainingSize} RPMem put failed due to time out, try again")
+          retry += 1
+          if (retry == 4) {
+            throw new IOException(
+              s"${cur_block_id}-${bufferRemainingSize} RPMem put failed due to time out.")
+          }
+        }
+        logDebug(s" [PUT Completed]${cur_block_id}-${bufferRemainingSize}")
+        key_id += 1
+        flushed_block_id = cur_block_id
+        cur_block_id = s"${blockId}_${key_id}"
+      } else {
+        val byteBuffer: ByteBuffer = buf.nioBuffer()
+        persistentMemoryWriter.setPartition(
+          numPartitions,
+          blockId,
+          byteBuffer,
+          bufferRemainingSize,
+          set_clean)
+      }
       bufferFlushedSize += bufferRemainingSize
       bufferRemainingSize = 0
     }
@@ -47,6 +72,10 @@ class PmemOutputStream(
     }
   }
 
+  def doFlush(): Unit = {
+
+  }
+
   def flushedSize(): Int = {
     bufferFlushedSize
   }
diff --git a/core/src/main/scala/org/apache/spark/util/collection/pmof/PmemExternalSorter.scala b/core/src/main/scala/org/apache/spark/util/collection/pmof/PmemExternalSorter.scala
index 1b462542..3c4e47d0 100644
--- a/core/src/main/scala/org/apache/spark/util/collection/pmof/PmemExternalSorter.scala
+++ b/core/src/main/scala/org/apache/spark/util/collection/pmof/PmemExternalSorter.scala
@@ -1,6 +1,7 @@
 package org.apache.spark.util.collection.pmof
 
 import java.util.Comparator
+import java.util.UUID
 
 import scala.collection.mutable
 import scala.collection.mutable.ArrayBuffer
@@ -12,11 +13,13 @@ import org.apache.spark.util.collection._
 import org.apache.spark.storage.pmof._
 import org.apache.spark.util.configuration.pmof.PmofConf
 import org.apache.spark.util.{CompletionIterator, Utils => TryUtils}
+import org.apache.spark.storage.BlockManager
 
 private[spark] class PmemExternalSorter[K, V, C](
     context: TaskContext,
     handle: BaseShuffleHandle[K, _, C],
     pmofConf: PmofConf,
+    blockManager: BlockManager,
     aggregator: Option[Aggregator[K, V, C]] = None,
     partitioner: Option[Partitioner] = None,
     ordering: Option[Ordering[K]] = None,
@@ -59,9 +62,10 @@ private[spark] class PmemExternalSorter[K, V, C](
     if (mapStage) {
       pmemBlockOutputStreamArray(partitionId)
     } else {
+      val tmpBlockName = s"${UUID.randomUUID()}"
       pmemBlockOutputStreamArray += new PmemBlockOutputStream(
         context.taskMetrics(),
-        PmemBlockId.getTempBlockId(stageId),
+        PmemBlockId.getTempBlockId(blockManager.blockManagerId.executorId, tmpBlockName, stageId),
         SparkEnv.get.serializerManager,
         serializer,
         SparkEnv.get.conf,
@@ -331,7 +335,18 @@ private[spark] class PmemExternalSorter[K, V, C](
   class SpillReader(pmemBlockOutputStream: PmemBlockOutputStream) {
     // Each spill reader is relate to one partition
     // which is different from spark original codes (relate to one spill file)
-    val pmemBlockInputStream = new PmemBlockInputStream[K, C](pmemBlockOutputStream, serializer)
+    val pmemBlockInputStream = if (!pmofConf.enableRemotePmem) {
+      new LocalPmemBlockInputStream[K, C](
+        pmemBlockOutputStream.getBlockId,
+        pmemBlockOutputStream.getTotalRecords,
+        serializer)
+    } else {
+      new RemotePmemBlockInputStream[K, C](
+        pmemBlockOutputStream.getBlockId,
+        pmemBlockOutputStream.mapStatus,
+        serializer,
+        pmofConf)
+    }
     var nextItem: (K, C) = _
 
     def readPartitionIter(partitionId: Int): Iterator[Product2[K, C]] = new Iterator[Product2[K, C]] {
diff --git a/core/src/main/scala/org/apache/spark/util/configuration/pmof/PmofConf.scala b/core/src/main/scala/org/apache/spark/util/configuration/pmof/PmofConf.scala
index fe8c0453..e39af6f1 100644
--- a/core/src/main/scala/org/apache/spark/util/configuration/pmof/PmofConf.scala
+++ b/core/src/main/scala/org/apache/spark/util/configuration/pmof/PmofConf.scala
@@ -29,4 +29,25 @@ class PmofConf(conf: SparkConf) {
   val pmemCoreMap = conf.get("spark.shuffle.pmof.dev_core_set", defaultValue = "/dev/dax0.0:0-17,36-53").split(";").map(_.trim).map(_.split(":")).map(arr => arr(0) -> arr(1)).toMap
   val fileEmptyTimeout: Int = conf.getInt("spark.shuffle.pmof.file_empty_timeout", defaultValue = 30)
   val fileEmptyInterval: Int = conf.getInt("spark.shuffle.pmof.file_empty_interval", defaultValue = 5)
+  val enableRemotePmem: Boolean = conf.getBoolean("spark.shuffle.pmof.enable_remote_pmem", defaultValue = false);
+  val enableRemotePmemSort: Boolean = conf.getBoolean("spark.shuffle.pmof.enable_remote_pmem_sort", defaultValue = false);
+  val rpmpHost: String = conf.get("spark.rpmp.rhost", defaultValue = "172.168.0.40")
+  val rpmpPort: String = conf.get("spark.rpmp.rport", defaultValue = "61010")
 }
+
+object PmofConf {
+  var ins: PmofConf = null
+  def getConf(conf: SparkConf): PmofConf =
+    if (ins == null) {
+      ins = new PmofConf(conf)
+      ins
+    } else {
+      ins
+    }
+  def getConf: PmofConf =
+    if (ins == null) {
+      throw new IllegalStateException("PmofConf is not initialized yet")
+    } else {
+      ins
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriterSuite.scala b/core/src/test/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriterSuite.scala
index 09740132..0bc8e16a 100644
--- a/core/src/test/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriterSuite.scala
+++ b/core/src/test/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriterSuite.scala
@@ -50,7 +50,7 @@ class PmemShuffleWriterSuite extends SparkFunSuite with SharedSparkContext with
     conf.set("spark.shuffle.pmof.pmem_list", "/dev/dax0.0")
     shuffleBlockResolver = new PmemShuffleBlockResolver(conf)
     serializer = new JavaSerializer(conf)
-    pmofConf = new PmofConf(conf)
+    pmofConf = PmofConf.getConf(conf)
     taskMetrics = new TaskMetrics()
     serializerManager = new SerializerManager(serializer, conf)
 
diff --git a/core/src/test/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriterWithSortSuite.scala b/core/src/test/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriterWithSortSuite.scala
index 15048d4f..5ec88ada 100644
--- a/core/src/test/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriterWithSortSuite.scala
+++ b/core/src/test/scala/org/apache/spark/shuffle/pmof/PmemShuffleWriterWithSortSuite.scala
@@ -55,7 +55,7 @@ class PmemShuffleWriterWithSortSuite extends SparkFunSuite with SharedSparkConte
     conf.set("spark.shuffle.pmof.pmem_list", "/dev/dax0.0")
     shuffleBlockResolver = new PmemShuffleBlockResolver(conf)
     serializer = new JavaSerializer(conf)
-    pmofConf = new PmofConf(conf)
+    pmofConf = PmofConf.getConf(conf)
     taskMetrics = new TaskMetrics()
     serializerManager = new SerializerManager(serializer, conf)
 
